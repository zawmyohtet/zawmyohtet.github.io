<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="/scripts/pretty-feed-v3.xsl" type="text/xsl"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:h="http://www.w3.org/TR/html4/"><channel><title>Zaw&apos;s Personal Blog</title><description>A Software Engineer with a passion for crafting high-performance software that&apos;s both robust and user-friendly</description><link>http://zawmyohtet.github.io</link><item><title>Agentic App Development Environment with Docker</title><link>http://zawmyohtet.github.io/blog/agentic-app/environment</link><guid isPermaLink="true">http://zawmyohtet.github.io/blog/agentic-app/environment</guid><pubDate>Wed, 02 Jul 2025 00:14:46 GMT</pubDate><content:encoded>&lt;p&gt;This guide will walk you through setting up a Python development environment, complete with &lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter&lt;/a&gt;, &lt;a href=&quot;https://www.langchain.com/langchain&quot;&gt;LangChain&lt;/a&gt;, &lt;a href=&quot;https://www.langchain.com/langgraph&quot;&gt;LangGraph&lt;/a&gt;, and &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;, all containerized with Docker. It&apos;ll also cover how to run a Large Language Model (LLM) locally using &lt;a href=&quot;https://ollama.com/&quot;&gt;Ollama&lt;/a&gt; and use together from Jupyter.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(A quick note: This tutorial is based on a macOS environment. While the core concepts apply universally, you might encounter minor differences depending on your operating system.)&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Before we begin, ensure you have the following installed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt;:&lt;/strong&gt;Â Essential for containerizing our development environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://ollama.com/&quot;&gt;Ollama&lt;/a&gt;:&lt;/strong&gt;Â For managing and running local LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Docker Environment Setup&lt;/h2&gt;
&lt;p&gt;Let&apos;s start by structuring our project. Create the following directories and files in your chosen project root:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;config/
docker-compose.yml
dockerfiles/
workspace/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This structure is a base on my personal preference, but feel free to adapt it to your preferred workflow.&lt;/p&gt;
&lt;p&gt;Next, create yourÂ &lt;strong&gt;Dockerfile&lt;/strong&gt;Â inside theÂ &lt;strong&gt;dockerfiles&lt;/strong&gt;Â directory. Name itÂ &lt;strong&gt;Jupyter.Dockerfile&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-docker&quot;&gt;# syntax=docker/dockerfile:1

FROM quay.io/jupyter/base-notebook

COPY ./dockerfiles/requirements.txt .

RUN pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the sameÂ dockerfilesÂ directory, create aÂ &lt;strong&gt;requirements.txt&lt;/strong&gt;Â file. This lists the Python packages that may be needed for our environment. It includes common dependencies for LangGraph and Kaggle, but you can customize it as required for your specific projects:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;langgraph
langchain-ollama
transformers
langgraph-prebuilt
langgraph-sdk
langgraph-checkpoint-sqlite
langsmith
langchain-community
langchain-core
langchain-openai
notebook
tavily-python
wikipedia
trustcall
langgraph-cli[inmem]
matplotlib
scikit-learn
pandas
kaggle
kagglehub
scipy
statsmodels
fastcluster
seaborn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&apos;s configure yourÂ &lt;strong&gt;docker-compose.yml&lt;/strong&gt;Â file in your main project directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-yml&quot;&gt;services:
  jupyter:
    build:
      context: .
      dockerfile: dockerfiles/Jupyter.Dockerfile
    tty: true
    container_name: jupyter
    ports:
      - 8888:8888 # Jupyter web UI
      - 2024:2024 # LangGraph Studio
    volumes:
      - ./workspace:/home/jovyan/work # Mount your project files
      - ./config:/home/jovyan/.config # For configuration files (e.g., Kaggle)
    command: start-notebook.py --NotebookApp.token=&apos;&amp;#x3C;replace_with_your_token&gt;&apos;
    networks:
      - app-network
volumes:
  jupyter-data:
    name: jupyter-data
  dev-cache:
    driver: local

networks:
  app-network:
    driver: bridge
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this configuration, we expose portsÂ &lt;strong&gt;8888&lt;/strong&gt;Â for Jupyter andÂ &lt;strong&gt;2024&lt;/strong&gt;Â for LangGraph Studio. TheÂ &lt;strong&gt;volumes&lt;/strong&gt;Â section ensures your project files inÂ &lt;strong&gt;workspace&lt;/strong&gt;Â and any configuration inÂ &lt;strong&gt;config&lt;/strong&gt;Â are accessible within the container.&lt;/p&gt;
&lt;p&gt;With the files in place, navigate to your project root in the terminal and start the Docker environment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose ps
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To access the command line within your Docker environment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose exec jupyter bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can access theÂ &lt;strong&gt;Jupyter web UI&lt;/strong&gt;Â viaÂ &lt;a href=&quot;https://127.0.0.1:8888/lab?token=%3Creplace_with_your_token%3E&quot;&gt;https://127.0.0.1:8888/lab?token=&amp;#x3C;replace_with_your_token&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LangGraph Studio&lt;/strong&gt;Â viaÂ &lt;a href=&quot;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&quot;&gt;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note: Before accessing LangGraph Studio, remember to start server from your project directoryÂ &lt;em&gt;within&lt;/em&gt;Â the Docker environment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;langgraph dev
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more detail about usage and setup, you can follow official &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Ollama for Local LLMs&lt;/h2&gt;
&lt;p&gt;Now, let&apos;s get an LLM running locally with Ollama. If Ollama is correctly installed, you can confirm its functionality:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ollama -h
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visit theÂ &lt;a href=&quot;https://ollama.com/&quot;&gt;Ollama website&lt;/a&gt;Â to browse available models. Choose a model that fits your system&apos;s resources. For instance, to download and run theÂ &lt;strong&gt;gemma3:1b&lt;/strong&gt;Â model locally:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ollama run gemma3:1b
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the model is loaded, you&apos;re ready to start leveraging your locally hosted LLM through Jupyter, typically by utilizing theÂ langchain_ollamaÂ package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from langchain_ollama import ChatOllama

llm = ChatOllama(
    model=&quot;gemma3:1b&quot;,
    temperature=0,
    base_url=&quot;http://host.docker.internal:11434&quot;
)
&lt;/code&gt;&lt;/pre&gt;</content:encoded><h:img src="/_astro/dependency-cloud.O13B6aDZ.png"/><enclosure url="/_astro/dependency-cloud.O13B6aDZ.png"/></item><item><title>My Quest for On-Device LLMs</title><link>http://zawmyohtet.github.io/blog/on-device-llm/on-device-llm</link><guid isPermaLink="true">http://zawmyohtet.github.io/blog/on-device-llm/on-device-llm</guid><description>An Apple &amp; Android Adventure!</description><pubDate>Mon, 23 Jun 2025 15:45:46 GMT</pubDate><content:encoded>&lt;p&gt;Have you ever wondered what it would be like to run powerful language models right on your phone? I certainly did! So, when Apple announced their &lt;a href=&quot;https://developer.apple.com/documentation/foundationmodels&quot;&gt;Foundation Model Framework&lt;/a&gt; at this year&apos;s WWDC, my ears perked up immediately. I was captivated â€“ how exactly would this work, and just how capable could it be?&lt;/p&gt;
&lt;p&gt;The very next morning, I eagerly downloaded the Xcode beta, dove into their documentation, and set out to build some on-device LLM magic. My excitement hit a snag, though. Turns out, it couldn&apos;t run on the emulator, and upgrading my iPhone and Mac to the latest developer beta wasn&apos;t an option for me at the moment. Sigh â€“ a minor setback!&lt;/p&gt;
&lt;p&gt;Undeterred, my attention soon shifted to Android when I discovered their ML Kit GenAI API. I decided to try out their &lt;a href=&quot;https://developers.google.com/ml-kit/genai/rewriting/android&quot;&gt;Rewriting API&lt;/a&gt;, only to face another hurdle: my Pixel 6a wasn&apos;t supported. Still, I was determined to see how convenient it would be to integrate ML Kit into an app. After a couple of hours (and a little help from Gemini for the UI!), I successfully integrated Android ML Kit into my app. While I finally got a taste of the developer experience, I was still itching for the actual user experience.&lt;/p&gt;
&lt;p&gt;My story didn&apos;t end there! I had the incredible opportunity to attend Google I/O Extended Singapore 2025. One of the speakers gave a fantastic talk about on-device language models, and that&apos;s where I learned about the Google Gen AI Edge Gallery project. As soon as I got home, I compiled the app, and finally, I could experience on-device GenAI firsthand!&lt;/p&gt;
&lt;p&gt;But my interest certainly didn&apos;t stop there. I decided to dig even deeper. Why not build something myself and truly get my hands dirty with the developer experience? This meant hours spent searching for the perfect model, grappling with the Android LLM Inference API, meticulously figuring out the UI, and dusting off my old Android development knowledge. So far, the UI is heavily inspired by the Google Gen AI Edge Gallery project and create the rewrite feature on my own way. I use &lt;a href=&quot;https://huggingface.co/litert-community/Gemma3-1B-IT/tree/main&quot;&gt;litert-community/Gemma3-1B-IT&lt;/a&gt; model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;./screenshot_20250624_001128.png&quot; alt=&quot;Device&quot;&gt;&lt;/p&gt;
&lt;p&gt;To Sum Up My On-Device LLM Journey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both Android and iOS platforms good developer experience.&lt;/li&gt;
&lt;li&gt;Both Apple&apos;s Foundation Model Framework and Android&apos;s ML Kit APIs are quite user-friendly, though I believe the documentation could definitely be improved.&lt;/li&gt;
&lt;li&gt;If it works as expected on a wider range of devices, Apple&apos;s support for structured outputs and tool calling from their Foundation Model Framework will be a real game-changer.&lt;/li&gt;
&lt;li&gt;The Android LLM Inference API, in its current state, seems quite raw. Apps still need to load their own models, which might not be practical for widespread use and could ultimately lead back to relying on hosted LLM services.&lt;/li&gt;
&lt;li&gt;Gemma3 1B model can run on my Google Pixel 6a device pretty well. I haven&apos;t tried complicated task but the current result meet my expectation. (Hay! better don&apos;t expect too high on such small model, right?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is just the beginning of my exploration! I still have so much more to dig into, and I&apos;m excited to share my new findings with you all soon!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğ˜—ğ˜³ğ˜°ğ˜¶ğ˜¥ğ˜­ğ˜º ğ˜®ğ˜º ğ˜°ğ˜¸ğ˜¯ ğ˜µğ˜©ğ˜°ğ˜¶ğ˜¨ğ˜©ğ˜µğ˜´, ğ˜´ğ˜©ğ˜¢ğ˜³ğ˜±ğ˜¦ğ˜¯ğ˜¦ğ˜¥ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜µğ˜©ğ˜¦ ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜´ğ˜µğ˜¢ğ˜¯ğ˜¤ğ˜¦ ğ˜°ğ˜§ ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜ˆğ˜.&lt;/p&gt;
&lt;/blockquote&gt;</content:encoded><h:img src="undefined"/><enclosure url="undefined"/></item><item><title>Hello World!</title><link>http://zawmyohtet.github.io/blog/welcome/hello-world</link><guid isPermaLink="true">http://zawmyohtet.github.io/blog/welcome/hello-world</guid><description>It has been quite a long I havenâ€™t done any blogging.</description><pubDate>Sun, 22 Jun 2025 08:21:07 GMT</pubDate><content:encoded>&lt;p&gt;It has been quite a long I haven&apos;t done any blogging.&lt;/p&gt;
&lt;p&gt;My first blog posted on blogspot platform last 15 years ago. All my blog posts were about iOS back then. Then I moved to &lt;a href=&quot;https://wordpress.org/&quot;&gt;Wordpress&lt;/a&gt;. The reason behind was to learn how to use cpanel and cloud hosting. It was quite a challenging experience for a person living in a third world country which only had very slow internet. Anyhow, I created my own first wordpress theme, used one click installer from cpanel and setup domain without knowing how it works internally. But I believe it showed me what my passion is.&lt;/p&gt;
&lt;p&gt;After getting my first permanent job, I could no longer maintain my personal blog. The domain and hosting were expired and I stopped blogging. During that time, social media started to become popular and I believe people&apos;s attention moved to short and concise content rather than reading long blog posts.&lt;/p&gt;
&lt;p&gt;After a few years, static site generators became popular and github offers personal pages. I decided to setup one for myself. I used &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt; first. Then moved to &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;. Although I spent time setting up (a couple of themes, contents, environment, etc), I was not able to write regularly. My blog was dead internally although it is alive and online.&lt;/p&gt;
&lt;p&gt;Now, I am back. Let&apos;s see how far I can go!&lt;/p&gt;
&lt;p&gt;This blog uses &lt;a href=&quot;https://astro.build/&quot;&gt;Astro&lt;/a&gt; with &lt;a href=&quot;https://github.com/cworld1/astro-theme-pure&quot;&gt;Astro pure theme&lt;/a&gt; and hosts on &lt;a href=&quot;https://pages.github.com/&quot;&gt;Github pages&lt;/a&gt;&lt;/p&gt;</content:encoded><h:img src="/_astro/hello-world-thumbnail.B3TyIY1-.png"/><enclosure url="/_astro/hello-world-thumbnail.B3TyIY1-.png"/></item></channel></rss>